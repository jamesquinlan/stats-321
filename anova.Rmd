---
title: "One-way Analysis of Variance in R"
author: "James Quinlan"
date: "10/21/2021"
output:
  pdf_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## One-way ANOVA

ANalysis Of Variance (ANOVA or AOV) is used to determine if the means of two or more groups are the same (or different).  That is, the goal is to test the hypothesis that the means are equal for all groups.
In particular,
\[
H_0: \mu_1 = \mu_2 = \dots = \mu_k
\]
The alternative hypothesis states that at least one is different. 
\[
H_1: \mu_i \ne \mu_j \; \exists i \ne j. 
\]

ANOVA generalizes the t-test to 3 or more groups.  In practice, if 2 groups, use Student t-test.
NOTE: This article specifically addresses one-way ANOVA. There many 'ANOVAs' (e.g., two-way ANOVA, repeated sample ANOVA, etc.).  




### Assumptions of ANOVA

As usual with statistical tests, several assumptions must be satisfied before the findings can be interpreted. Although it is theoretically feasible to execute these tests when one or more assumptions are not satisfied, be cafeful interpreting the findings and stating conclusions. 


The following are the ANOVA assumptions, how to test them, and what alternate tests are available if an assumption is not met:


- **Variable Types**: One-way ANOVA involves a continuous quantitative dependent variable (corresponding to the measurements in the inquiry) and a qualitative independent variable (with at least 2 levels that will serve as the comparison groups). 


- **Independence**: The data should be independent across groups and within each group, acquired from a representative and randomly selected fraction of the overall population. Rather than a formal test, the assumption of independence is frequently validated based on the experiment design and excellent control of experimental variables. If you are still not confident about independence based on the experiment design, consider if one observation is connected to another (or whether one observation has an influence on another) inside each group or between groups. If you do not, you very certainly have independent samples. If the observations between samples (forming the different groups to be compared) are dependent (for example, if three measurements were taken on the same person, as is often the case in medical studies when measuring a metric I before, (ii) during, and (iii) after a treatment), the repeated measures ANOVA should be used to account for the dependency between the samples. 


- **Normality**: Residuals should have a normal distribution in general. A histogram and a QQ-plot may be used to test the normality assumption _visually_, and a normality test such as the Shapiro-Wilk or Kolmogorov-Smirnov can be used as a analytic test. If the residuals still do not follow a normal distribution following a transformation (e.g., logarithmic transformation, square root, Box-Cox, etc. ), the Kruskal-Wallis test (Kruskal.test(variable group, data = data in R) can be used. This non-parametric test, resistant to non-normal distributions, has the same purpose as the ANOVA: to compare three or more groups, but it compares groups using sample medians rather than sample means. 

- **Homoscedasticity** (or homogeneity of the variances):  in populations, the variances of distinct groups should be equal. This assumption may be verified graphically (by comparing the dispersion in a boxplot or dot plot, for example) or more formally (by using the Levene's test (Levene test(variable g) from the car package) or Bartlett's test, for example). If the equal variance hypothesis is rejected, another ANOVA may be used: the Welch ANOVA (oneway.test(variable group, var.equal = FALSE)). Although the Welch ANOVA does not need homogeneity among the variances, the distributions should still follow a normal pattern. It is worth noting that the Kruskal-Wallis test does not need normalcy or homoscedasticity of variances. 

- **No Outliers**: An outlier is a value or observation that differs significantly from the others. If there are any significant outliers in the different groups, your ANOVA results may be skewed. There are various approaches for detecting outliers in your data, but you must choose one of the following to deal with them: 

  * use a non-parametric variant (i.e., the Kruskal-Wallis test)
  
  * transform your data (e.g., logarithmic or Box-Cox transformations) 
  
  * delete them 









### Set up 
Before we begin, let's set the working directory and load libraries.  

#### Load Libraries
Load the `tidyverse` library ,although the two main libraries we use from `tidyvere` are `dpylr` and `ggplot2`.  For more information about tidyverse see: https://dplyr.tidyverse.org/ .

```{r, echo=TRUE}
library(tidyverse)
```



### Data: Load, Clean, Wrangle

In this article, we will use a data set that contains information on 78 people using one of three diets to determine which diet is best for losing weight. We test to see if there is a difference in average weight loss between the three different diets. This data is obtained from the [Mathematics and Statistics Help (MASH) site](https://www.sheffield.ac.uk/mash/statistics/datasets) at the University of Sheffield.  Download and save to the current working directory.
Obtain the path to the working directory using the command `getwd()`.  Set the 
working directory with the function `setwd()`. 

```{r, echo=FALSE}
folder = '/data/'
```



```{r}
# Load the data
raw = read_csv(paste(getwd(),folder,'diet.csv',sep=''))
```
Extract the data of interest.  In this case, we will need to create a new variable called `Loss` which is the difference between the subjects weight before beginning the diet and then six weeks after.

```{r}
Data = raw %>% 
  mutate(Loss = pre.weight - weight6weeks) %>%
  select(Diet,Loss)
```
Attach the data to work with non fully qualified names, e.g., `Loss` vs. `Data$Loss`.  

```{r}
attach(Data)
# Don't forget to detach("Data")
```

Convert the `Diet` variable to a factor.  
```{r}
Data$Diet = as.factor(Data$Diet)
```

Split the loss by group for later use, if needed.
```{r}
Group1 = Data %>% select(Diet,Loss) %>% filter(Diet==1)
Group2 = Data %>% select(Diet,Loss) %>% filter(Diet==2)
Group3 = Data %>% select(Diet,Loss) %>% filter(Diet==3)
```



#### Summary Statistics
After loading and cleaning the data, always run summary statistics before proceeding with statistical tests and analysis.


```{r}
# Summary of Data by variable (Diet and Loss)
summary(Data)
```


Summary by diet, 

```{r}
Data %>% 
  group_by(Diet) %>% 
  summarize(
    min = min(Loss), 
    q1 = quantile(Loss, 0.25), 
    median = median(Loss), 
    mean = mean(Loss), 
    q3 = quantile(Loss, 0.75), 
    max = max(Loss)
  )

# Alternatively
# tapply(Data$Loss, Data$Diet, summary)
```

We see Diet 3 seems to be the "best" with both the largest average weight lost and maximum weight lost.  In addition, Diet 1 and 2 saw some weight gain.  



#### Plots
```{r}
ggplot(Data) +
  aes(x = Diet, y = Loss, color = Diet) +
  geom_jitter() +
  theme_classic()
```
 
We can also look at a boxplot.  When we check for homogeneity we will revisit the boxplot.

```{r}
# Boxplot
boxplot(Loss ~ Diet,
  data = Data,
  main = "Weight Loss",
  xlab = "Diet",
  ylab = "Loss",
  col = "steelblue",
  border = "black"
)
```
 
 





  
Choosing the right test based on whether or not assumptions are satisfied might be complex, so here's a quick rundown: 


* Verify the observations are independent by checking the design of the study 
* Test the normality of residuals 
* If normal residuals, test the homogeneity of the variances 
  - If variance are equal then use ANOVA 
  - If unequal variances, then use the Welch ANOVA 
* If not normally distributed residuals, then use the Kruskal-Wallis test 









Lets examine the underlying assumptions of our particular dataset one by one.



1. __Independence__ – the observations in each group need to be independent of each other. Since we used a randomized design (i.e. we assigned participants to the exercise programs randomly), this assumption should be met so we don’t need to worry too much about this.


ANOVA assumes:

    The observations in each group are independent of the observations in every other group.
    The observations within each group were obtained by a random sample.

How to check this assumption:  Durbin-Watson test (see below after running the ANOVA model).





NOTE: Chi-square Test for Independence is not appropriate because both variables are not categorical.





### __Normality Tests__


the dependent variable should be approximately normally distributed for each level of the predictor variable. 

<!-- 
https://statsandr.com/blog/do-my-data-follow-a-normal-distribution-a-note-on-the-most-widely-used-distribution-and-how-to-test-for-normality-in-r/#normality-test  
-->


```{r}
hist(Data$Loss,
  xlab = "Loss",
  main = "Histogram of Loss",
  breaks = sqrt(nrow(Data))
) # set number of bins
```



```{r}
ggplot(Data) +
  aes(x = Loss) +
  geom_histogram(bins = 10, fill = "#336699") +
  theme_minimal()
```





```{r}
# Draw points on the qq-plot:
qqnorm(Data$Loss)
# Draw the reference line:
qqline(Data$Loss)
```


```{r}
qplot(
  sample = Loss, data = Data,
  col = Diet, shape = Diet
)
```

```{r}
ggplot(Data) +
  aes(x = Loss) +
  geom_density()
```



#### Shapiro-Wilk's Test for Nomrality

Shapiro-Wilk's and Kolmogorov-Smirnov are two statistical tests of normality.  The null hypothesis is, 
\[
H_0: \text{the data is normally distributed}
\]
Of course, the alterative hypothesis is, $H_1$: the data is **not** normally distributed.  Here we employ the Shapiro-Wilk test.  <!--  Shapiro-Wilk test is known to have better power than Kolmogorov-Smirnov test.  -->

```{r}
shapiro.test(Data$Loss)
```





















### Homoscedasticity   

ANOVA assumes homogeneity in the population variances, that is, variances are equal.
How to check this assumption in R:

We can check this assumption in R using:

* Numerically with summary data


```{r}
cbind(var(Group1$Loss), var(Group2$Loss), var(Group3$Loss))
```



* Visually using plots(i.e., boxplots).

```{r}
boxplot(Loss ~ Diet)
```

* Formal test statistic (i.e., See Bartlett’s Test below).


#### Bartlett’s Test.

Create box plots that show distribution of weight loss for each group

```{r}
bartlett.test(Loss ~ Diet, data=Data)
```










### __Outliers__
<!-- Source: https://statsandr.com/blog/outliers-detection-in-r/ -->

Statistical tests for outliers include: Grubb's test, Dixon's test (small samples $n \le 25$), and Rosner's test (detect multiple outliers).


#### Grubb's Test
To test if the single highest value is an outlier, i.e., 

$H_0$: Highest value is NOT an outlier 

$H_1$: Highest value IS an outlier



```{r}
# install.packages("outliers")
library(outliers)
test.outliers = grubbs.test(Data$Loss)
test.outliers
```

```{r}
OUT = boxplot.stats(Group1$Loss)$out
todrop = which(Data$Loss > 8.49 & Data$Diet == 1)
#Data2 <- Data[-todrop,]
```


















## ANOVA in R
```{r}
#fit the one-way ANOVA model
model = aov(Loss ~ Diet, data = Data)

#view the model output
#summary(model)
```

```{r}
summary(model)
```




Test the independence of observations.  If `DW` is between 1.5 and 2.5 then autocorrelation is likely not a cause for concern.  The Durbin-Watson test uses the following hypotheses:
H0 (null hypothesis): There is no correlation among the residuals.
HA (alternative hypothesis): The residuals are autocorrelated.

```{r}
# install.packages("lmtest")
library(lmtest)    # linear model tests
dwtest(model)      # Durbin-Watson test  
# plot(model)
```